# -*- coding: utf-8 -*-
"""transfer_learning_shit_done.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ygtNvwQ7OJEG9JYab0lompRqvdwGQRGs
"""

import torch, torchvision

from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import cv2
import pandas as pd
import tqdm as tqdm
import PIL.Image as Image
import seaborn as sns
from pylab import rcParams
from matplotlib import rc
from matplotlib.ticker import MaxNLocator
from torch.optim import lr_scheduler
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils, datasets, models
from glob import glob
import shutil
from collections import defaultdict
from torch import nn, optim
import torch.nn.functional as F
import torchvision.transforms as T
from torchvision.datasets import ImageFolder
from torchvision import models

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format='retina'
sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 12, 8
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

!wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip

!unzip -qq GTSRB_Final_Training_Images.zip

train_folders = sorted(glob('GTSRB/Final_Training/Images/*'))
len(train_folders)

def load_images(img_path, resize=True):
    img = cv2.imread(img_path)
    if img is None:
        raise ValueError(f"Image not found or failed to load: {img_path}")
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    if resize:
        img = cv2.resize(img, (224, 224))
    return img

def show_images(img_paths):
    img = load_images(img_paths)
    plt.imshow(img)
    plt.axis('off')

def show_sign_grid(image_paths):
  images = [load_images(image_path) for image_path in image_paths]
  images = torch.as_tensor(images)
  images = images.permute(0, 3, 1, 2)
  grid_img = torchvision.utils.make_grid(images, nrow=10)
  plt.figure(figsize=(24, 12))
  plt.imshow(grid_img.permute(1, 2, 0))
  plt.axis('off');

sample_images = [np.random.choice(glob(train_folder + '/*.ppm')) for train_folder in train_folders]
show_sign_grid(sample_images)

img_path = '/content/GTSRB/Final_Training/Images/00000/00000_00000.ppm'
show_images(img_path)

class_names = ['priority_road', 'give_way', 'stop', 'no_entry']
class_indices = [12, 13, 14, 17]

! mkdir data

data_dir = Path('data')

Datasets = ["train", "valid", "test"]

for ds in Datasets:
    for cls in class_names:
        (data_dir / ds / cls).mkdir(parents=True, exist_ok=True)

for i, cls_index in enumerate(class_indices):
    image_paths = np.array(glob(f'{train_folders[cls_index]}/*.ppm'))
    class_name = class_names[i]
    print(f'{class_name}: {len(image_paths)}')

    np.random.shuffle(image_paths)
    ds_split = np.split(
    image_paths,
    indices_or_sections=[int(.8*len(image_paths)), int(.9*len(image_paths))])

    dataset_data = zip(Datasets, ds_split)
    for ds, split in dataset_data:
        for img_path in split:
            shutil.copy(img_path, data_dir / ds / class_name)

### image augmantation techniques

mean_nums = [0.485, 0.456, 0.406]
std_nums = [0.229, 0.224, 0.225]

transforms = {'train': T.Compose([
    T.RandomResizedCrop(size=256),
    T.RandomRotation(degrees=15),
    T.RandomHorizontalFlip(),
    T.ToTensor(),
    T.Normalize(mean_nums, std_nums)
    ]), 'valid': T.Compose([
    T.Resize(size=256),
    T.CenterCrop(size=224),
    T.ToTensor(),
    T.Normalize(mean_nums, std_nums)
    ]), 'test': T.Compose([
    T.Resize(size=256),
    T.CenterCrop(size=224),
    T.ToTensor(),
    T.Normalize(mean_nums, std_nums)
    ]),
  }

image_datasets = {d: ImageFolder(f'{data_dir}/{d}', transforms[d]) for d in Datasets}

data_loaders = {d: torch.utils.data.DataLoader(image_datasets[d], batch_size=4, shuffle=True, num_workers=4) for d in Datasets}

image_datasets

data_loaders

datasets_sizes = {d: len(image_datasets[d]) for d in Datasets}
class_names = image_datasets['train'].classes

datasets_sizes

def imshow(inp, title=None):
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([mean_nums])
    std = np.array([std_nums])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)

    if title is not None:
      plt.title(title)

    plt.axis('off')

inputs, classes = next(iter(data_loaders['train']))
out = torchvision.utils.make_grid(inputs)

imshow(out, title=[class_names[x] for x in classes])

def create_model(num_classes):
  model = models.resnet34(pretrained=True)
  n_features = model.fc.in_features
  model.fc = nn.Linear(n_features, num_classes)
  return model.to(device)

base_model = create_model(num_classes=4)

def train_epoch(model, dataloader, loss_fn, optimizer, device, scheduler, n_examples):
  model = model.train()

  losses = []
  correct_predictions = 0

  for inputs, labels in tqdm.tqdm(dataloader):
    inputs = inputs.to(device)
    labels = labels.to(device)

    outputs = model(inputs)

    _, preds = torch.max(outputs, dim=1)
    loss = loss_fn(outputs, labels)

    correct_predictions += torch.sum(preds == labels)

    losses.append(loss.item())

    loss.backward()

    optimizer.step()
    optimizer.zero_grad()

  scheduler.step()

  return correct_predictions.double() / n_examples, np.mean(losses)

def eval_model(model, dataloader, loss_fn, device, n_examples):
  model = model.eval()

  losses = []
  correct_predictions = 0

  with torch.no_grad():
    for inputs, labels in tqdm.tqdm(dataloader):
      inputs = inputs.to(device)
      labels = labels.to(device)

      outputs = model(inputs)

      _, preds = torch.max(outputs, dim=1)

      loss = loss_fn(outputs, labels)

      correct_predictions += torch.sum(preds == labels)
      losses.append(loss.item())

  return correct_predictions.double() / n_examples, np.mean(losses)

def train_model(model, dataloaders_dict, device, num_epochs):
  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
  scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
  loss_fn = nn.CrossEntropyLoss().to(device)

  history = defaultdict(list)
  best_acc = 0.0
  best_epoch = 0

  for epoch in range(num_epochs):
    print(f"epoch {epoch + 1}/{num_epochs}")
    print('-' * 10)

    train_acc, train_loss = train_epoch(model, dataloaders_dict['train'], loss_fn, optimizer, device, scheduler, datasets_sizes['train'])
    valid_acc, valid_loss = eval_model(model, dataloaders_dict['valid'], loss_fn, device, datasets_sizes['valid'])

    print(f'train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')
    print(f'valid loss: {valid_loss:.4f}, valid acc: {valid_acc:.4f}')
    print()

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['valid_acc'].append(valid_acc)
    history['valid_loss'].append(valid_loss)

    if valid_acc > best_acc:
      best_acc = valid_acc
      best_epoch = epoch + 1

      torch.save(model.state_dict(), 'best_model_state.bin')
  print(f'best valid acc: {best_acc}, best epoch: {best_epoch}')

  model.load_state_dict(torch.load('best_model_state.bin'))
  return model, history

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# base_model, history = train_model(base_model, data_loaders, device, num_epochs=10)

def plot_training_history(history):
  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
  ax1.plot(history['train_loss'], label='train loss')
  ax1.plot(history['valid_loss'], label='valid loss')

  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
  ax1.set_ylabel('loss')
  ax1.set_xlabel('epoch')
  ax1.set_title('loss history')
  ax1.legend()

  ax2.plot(history['train_acc'], label='train acc')
  ax2.plot(history['valid_acc'], label='valid acc')

  ax2.xaxis.set_major_locator(MaxNLocator(integer=True))
  ax2.set_ylabel('accuracy')
  ax2.set_xlabel('epoch')
  ax2.set_title('accuracy history')
  ax2.legend()

  fig.suptitle('training history')
  plt.show()

plot_training_history(history)

base_model.load_state_dict(torch.load('best_model_state.bin'))

def show_predictions(model, class_names, n=6):
    model = model.eval()
    images_handled = 0
    plt.figure()
    with torch.no_grad():
        for i, (inputs, labels) in enumerate(data_loaders['test']):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_handled += 1
                ax = plt.subplot(n, n, images_handled)
                ax.axis('off')
                ax.set_title(f'predicted: {class_names[preds[j]]}', fontsize=8)  # Smaller font
                imshow(inputs.cpu().data[j])

                if images_handled == n * n:
                    return

show_predictions(base_model, class_names, n=8)

def get_predictions(model, dataloader):
    model = model.eval()
    predictions = []
    real_values = []
    with torch.no_grad():
        for inputs, labels in tqdm.tqdm(dataloader):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            predictions.extend(preds)
            real_values.extend(labels)
    predictions = torch.as_tensor(predictions).cpu()
    real_values = torch.as_tensor(real_values).cpu()
    return predictions, real_values

y_pred, y_test = get_predictions(base_model, data_loaders['test'])

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred, target_names=class_names))

def show_confusion_matrix(confusion_matrix, class_names):

  cm = confusion_matrix.copy()

  cell_counts = cm.flatten()
  cm_row_norm = cm / cm.sum(axis=1)[:, np.newaxis]

  row_percentages = ["{0:.2f}".format(value) for value in cm_row_norm.flatten()]

  cell_labels = [f"{cnt}\n{per}" for cnt, per in zip(cell_counts, row_percentages)]
  cell_labels = np.asarray(cell_labels).reshape(cm.shape[0], cm.shape[1])

  df_cm = pd.DataFrame(cm_row_norm, index=class_names, columns=class_names)

  hmap = sns.heatmap(df_cm, annot=cell_labels, fmt='', cmap='Blues')
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True label')
  plt.xlabel('Predicted label');

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
show_confusion_matrix(cm, class_names)

#### check the reall world images

! gdown --id 19Qz3a61Ou_QSHsLeTznx8LtDBu4tbqHr

show_images('stop-sign.jpg')

## check for the confidence of each class

def predict_proba(model, img_path):
    img = Image.open(img_path)
    img = img.convert('RGB')
    img = transforms['test'](img).unsqueeze(0)

    pred = model(img.to(device))
    print(pred)
    pred = F.softmax(pred, dim=1)
    return pred.detach().cpu().numpy()

pred = predict_proba(base_model, 'stop-sign.jpg')
pred

pred_df = pd.DataFrame(pred, columns=class_names)
pred_df

sns.barplot(x="values", y="class_name", data=pred_df.melt(var_name="class_name", value_name="values"))
plt.xlim([0, 1])

### the traffic sign which never seen before

! gdown --id 1F61-iNhlJk-yKZRGcu6S9P29HxDFxF0u

show_images('unknown-sign.jpg')

pred = predict_proba(base_model, 'unknown-sign.jpg')
pred

pred_df = pd.DataFrame(pred, columns=class_names)
pred_df

sns.barplot(x="values", y="class_name", data=pred_df.melt(var_name="class_name", value_name="values"))
plt.xlim([0, 1])

## the sbove result is wrong
#### so we need to add a class of unknown for this one

unknown_indices = [
    i for i, f in enumerate(train_folders) if i not in class_indices
]

len(unknown_indices)

for ds in Datasets:
    (data_dir / ds / 'unknown').mkdir(parents=True, exist_ok=True)

    for ui in unknown_indices:
        image_paths = np.array(glob(f'{train_folders[ui]}/*.ppm'))
        image_paths = np.random.choice(image_paths, 50)

        ds_split = np.split(image_paths, indices_or_sections=[int(.8*len(image_paths)), int(.9*len(image_paths))])

        dataset_data = zip(Datasets, ds_split)
        for ds, split in dataset_data:
            for img_path in split:
                shutil.copy(img_path, data_dir / ds / 'unknown')

image_datasets = {d: ImageFolder(f'{data_dir}/{d}', transforms[d]) for d in Datasets}
data_loaders = {d: torch.utils.data.DataLoader(image_datasets[d], batch_size=4, shuffle=True, num_workers=4) for d in Datasets}

dataset_sizes = {d: len(image_datasets[d]) for d in Datasets}
class_names = image_datasets['train'].classes
dataset_sizes

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# enhanced_model = create_model(num_classes=5)
# enhanced_model, history = train_model(enhanced_model, data_loaders, dataset_sizes, device)

plot_training_history(history)

show_images('unknown-sign.jpg')

pred = predict_proba(enhanced_model, 'unknown-sign.jpg')
show_prediction_confidence(enhanced_model, class_names)

show_predictions(enhanced_model, class_names, n_images=8)

y_pred, y_rest = get_predictions(enhanced_model, data_loaders['test'])

print(classification_report(y_test, y_pred, target_names=class_names))

cm = confusion_matrix(y_test, y_pred)
show_confusion_matrix(cm, class_names)

